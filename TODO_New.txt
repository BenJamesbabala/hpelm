

Final goals

    1. Solve huge tasks distributively
    2. Use any types of neurons
    3. Use regularizations and prunings
    4. Use classification

######################################################################


List of desired behaviour

    1. Solve XOR problem with 'sigm' ELM
    2. Solve Sine problem with 'sigm' ELM
    3. Handle inputs normalization (parameters are given)
    4. Types of single input neurons: linear, 'sigm', 'tanh'
    5. Types of two-input neurons: 'rbf_l2', 'rbf_l1', 'rbf_linf'
    6. Read data from files: text and binary Numpy
    7. Save and load ELM model
    


List of advanced behaviour - general ELM model

    1. Normalizations in solution: TROP
    2. Accelerated neuron computing 
    3. Matlab integration
    4. Proper multiclass support
    5. ??? Arbitrary neurons
    6. !!! Timeseries !!!



List of advanced desired behaviour - small data ELM

    1. MSE_PRESS error for classification
    2. Neuron pruning: MRSR, MRSR2



List of advanced desired behaviour - huge data ELM

    1. Based on HDF5 data
    2. Distributed computing of the result
    3. Distributed MSE error for classification
    4. High performance solvers (MAGMA)
    5. Information criteria for neuron pruning
    6. ??? Parallel I/O



List of additional tools

    1. Normalization parameters calculator
    2. Class encoder/decoder
    3. RBF neurons sampler: random, given, kNN
    4. Kernel neurons helper
    5. Get data about saved ELM model


#####################################################################


    H = []
    for neuron in neurons:
        H0 = project(X, neuron.W, neuron.b, neuron.kind)
        H0 = neuron.ufunc(H0)
        H.append(H0)
    H = np.hstack(H)        

    # project(X,W,b,"sigm") = X.dot(W) + b
    # project(X,W,b,"rbf_l2") = cdist(X,W,"l2") / -2*(b**2)

