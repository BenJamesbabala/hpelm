

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>hpelm package &mdash; hpelm 0.6.22 documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Roboto+Slab:400,700|Inconsolata:400,700&subset=latin,cyrillic' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="hpelm 0.6.22 documentation" href="index.html"/>
        <link rel="next" title="hpelm.modules package" href="hpelm.modules.html"/>
        <link rel="prev" title="Welcome to hpelm’s documentation!" href="index.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        
          <a href="index.html" class="fa fa-home"> hpelm</a>
        
        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="">hpelm package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hpelm.elm">hpelm.elm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hpelm.hp_elm">hpelm.hp_elm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hpelm.mss_cv">hpelm.mss_cv module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hpelm-mss-hpv-module">hpelm.mss_hpv module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hpelm.mss_loo">hpelm.mss_loo module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hpelm.mss_v">hpelm.mss_v module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-hpelm">Module contents</a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">hpelm</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>hpelm package</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/hpelm.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <div class="section" id="hpelm-package">
<h1>hpelm package<a class="headerlink" href="#hpelm-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="hpelm.modules.html">hpelm.modules package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hpelm.modules.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.modules.html#module-hpelm.modules.hdf5_tools">hpelm.modules.hdf5_tools module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.modules.html#module-hpelm.modules.mrsr">hpelm.modules.mrsr module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.modules.html#module-hpelm.modules.mrsr2">hpelm.modules.mrsr2 module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.modules.html#module-hpelm.modules.rbf_param">hpelm.modules.rbf_param module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.modules.html#module-hpelm.modules">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hpelm.nnets.html">hpelm.nnets package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hpelm.nnets.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.nnets.html#module-hpelm.nnets.slfn">hpelm.nnets.slfn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.nnets.html#hpelm-nnets-slfn-python-module">hpelm.nnets.slfn_python module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.nnets.html#hpelm-nnets-slfn-skcuda-module">hpelm.nnets.slfn_skcuda module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.nnets.html#module-hpelm.nnets">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hpelm.tests.html">hpelm.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hpelm.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.tests.html#module-hpelm.tests.test_acceptance">hpelm.tests.test_acceptance module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.tests.html#module-hpelm.tests.test_corr_hpelm">hpelm.tests.test_corr_hpelm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.tests.html#module-hpelm.tests.test_correctness">hpelm.tests.test_correctness module</a></li>
<li class="toctree-l2"><a class="reference internal" href="hpelm.tests.html#module-hpelm.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hpelm.elm">
<span id="hpelm-elm-module"></span><h2>hpelm.elm module<a class="headerlink" href="#module-hpelm.elm" title="Permalink to this headline">¶</a></h2>
<p>Created on Mon Oct 27 17:48:33 2014</p>
<p>&#64;author: akusok</p>
<dl class="class">
<dt id="hpelm.elm.ELM">
<em class="property">class </em><code class="descclassname">hpelm.elm.</code><code class="descname">ELM</code><span class="sig-paren">(</span><em>inputs</em>, <em>outputs</em>, <em>classification=''</em>, <em>w=None</em>, <em>batch=1000</em>, <em>accelerator=None</em>, <em>precision='double'</em>, <em>norm=None</em>, <em>tprint=5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Interface for training Extreme Learning Machines (ELM).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>inputs</strong> (<em>int</em>) &#8211; dimensionality of input data, or number of data features</li>
<li><strong>outputs</strong> (<em>int</em>) &#8211; dimensionality of output data, or number of classes</li>
<li><strong>classification</strong> (<em>&#8216;c&#8217;/&#8217;wc&#8217;/&#8217;ml&#8217;, optional</em>) &#8211; train ELM for classfication (&#8216;c&#8217;) / weighted classification (&#8216;wc&#8217;) /
multi-label classification (&#8216;ml&#8217;). For weighted classification you can provide weights in <cite>w</cite>. ELM will
compute and use the corresponding classification error instead of Mean Squared Error.</li>
<li><strong>w</strong> (<em>vector, optional</em>) &#8211; weights vector for weighted classification, lenght (<cite>outputs</cite> * 1).</li>
<li><strong>batch</strong> (<em>int, optional</em>) &#8211; batch size for data processing in ELM, reduces memory requirements. Does not work
for model structure selection (validation, cross-validation, Leave-One-Out). Can be changed later directly
as a class attribute.</li>
<li><strong>accelerator</strong> (<em>string, optional</em>) &#8211; type of accelerated ELM to use: None, &#8216;GPU&#8217;, ...</li>
<li><strong>precision</strong> (<em>optional</em>) &#8211; data precision to use, supports signle (&#8216;single&#8217;, &#8216;32&#8217; or numpy.float32) or double
(&#8216;double&#8217;, &#8216;64&#8217; or numpy.float64). Single precision is faster but may cause numerical errors. Majority
of GPUs work in single precision. Default: <strong>double</strong>.</li>
<li><strong>norm</strong> (<em>double, optinal</em>) &#8211; L2-normalization parameter, <strong>None</strong> gives the default value.</li>
<li><strong>tprint</strong> (<em>int, optional</em>) &#8211; ELM reports its progess every <cite>tprint</cite> seconds or after every batch,
whatever takes longer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Class attributes; attributes that simply store initialization or <cite>train()</cite> parameters are omitted.</p>
<dl class="attribute">
<dt id="hpelm.elm.ELM.nnet">
<code class="descname">nnet</code><a class="headerlink" href="#hpelm.elm.ELM.nnet" title="Permalink to this definition">¶</a></dt>
<dd><p><em>object</em></p>
<p>Implementation of neural network with computational methods, but without
complex logic. Different implementations are given by different classes: for Python, for GPU, etc.
See <code class="docutils literal"><span class="pre">hpelm.nnets</span></code> folder for particular files. You can implement your own computational algorithm
by inheriting from <code class="docutils literal"><span class="pre">hpelm.nnets.SLFN</span></code> and overwriting some methods.</p>
</dd></dl>

<dl class="attribute">
<dt id="hpelm.elm.ELM.flist">
<code class="descname">flist</code><a class="headerlink" href="#hpelm.elm.ELM.flist" title="Permalink to this definition">¶</a></dt>
<dd><p><em>list of strings</em></p>
<p>Awailable types of neurons, use them when adding new neurons.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Below the &#8216;matrix&#8217; type means a 2-dimensional Numpy.ndarray.</p>
</div>
<dl class="method">
<dt id="hpelm.elm.ELM.add_data">
<code class="descname">add_data</code><span class="sig-paren">(</span><em>X</em>, <em>T</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.add_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.add_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed new training data (X,T) to ELM model in batches; does not solve ELM itself.</p>
<p>Helper method that updates intermediate solution parameters HH and HT, which are used for solving ELM later.
Updates accumulate, so this method can be called multiple times with different parts of training data.
To reset accumulated training data, use <cite>ELM.nnet.reset()</cite>.</p>
<p>For training an ELM use <cite>ELM.train()</cite> instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> (<em>matrix</em>) &#8211; input training data</li>
<li><strong>T</strong> (<em>matrix</em>) &#8211; output training data</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.add_neurons">
<code class="descname">add_neurons</code><span class="sig-paren">(</span><em>number</em>, <em>func</em>, <em>W=None</em>, <em>B=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.add_neurons"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.add_neurons" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds neurons to ELM model. ELM is created empty, and needs some neurons to work.</p>
<p>Add neurons to an empty ELM model, or add more neurons to a model that already has some.</p>
<p>Random weights <cite>W</cite> and biases <cite>B</cite> are generated automatically if not provided explicitly.
Maximum number of neurons is limited by the available RAM and computational power, a sensible limit
would be 1000 neurons for an average size dataset and 15000 for the largest datasets. ELM becomes slower after
3000 neurons because computational complexity is proportional to a qube of number of neurons.</p>
<p>This method checks and prepares neurons, they are actually stored in <cite>solver</cite> object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>number</strong> (<em>int</em>) &#8211; number of neurons to add</li>
<li><strong>func</strong> (<em>string</em>) &#8211; type of neurons: &#8220;lin&#8221; for linear, &#8220;sigm&#8221; or &#8220;tanh&#8221; for non-linear,
&#8220;rbf_l1&#8221;, &#8220;rbf_l2&#8221; or &#8220;rbf_linf&#8221; for radial basis function neurons.</li>
<li><strong>W</strong> (<em>matrix, optional</em>) &#8211; random projection matrix size (<cite>inputs</cite> * <cite>number</cite>). For &#8216;<a href="#id1"><span class="problematic" id="id2">rbf_</span></a>&#8216; neurons,
W stores centroids of radial basis functions in transposed form.</li>
<li><strong>B</strong> (<em>vector, optional</em>) &#8211; bias vector of size (<cite>number</cite> * 1), a 1-dimensional Numpy.ndarray object.
For &#8216;<a href="#id3"><span class="problematic" id="id4">rbf_</span></a>&#8216; neurons, B gives widths of radial basis functions.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.confusion">
<code class="descname">confusion</code><span class="sig-paren">(</span><em>T</em>, <em>Y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.confusion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.confusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes confusion matrix for classification.</p>
<p>Confusion matrix <span class="math">\(C\)</span> such that element <span class="math">\(C_{i,j}\)</span> equals to the number of observations known
to be class <span class="math">\(i\)</span> but predicted to be class <span class="math">\(j\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>T</strong> (<em>matrix</em>) &#8211; true outputs or classes, size (N * <cite>outputs</cite>)</li>
<li><strong>Y</strong> (<em>matrix</em>) &#8211; predicted outputs by ELM model, size (N * <cite>outputs</cite>)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>conf</strong> &#8211;
confusion matrix, size (<cite>outputs</cite> * <cite>outputs</cite>)</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">matrix</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.error">
<code class="descname">error</code><span class="sig-paren">(</span><em>T</em>, <em>Y</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.error"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.error" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate error of model predictions.</p>
<p>Computes Mean Squared Error (MSE) between model predictions Y and true outputs T.
For classification, computes mis-classification error.
For multi-label classification, correct classes are all with Y&gt;0.5.</p>
<p>For weighted classification the error is an average weighted True Positive Rate,
or percentage of correctly predicted samples for each class, multiplied by weight
of that class and averaged. If you want something else, just write it yourself :)
See <a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">https://en.wikipedia.org/wiki/Confusion_matrix</a> for details.</p>
<p>Another option is to use scikit-learn&#8217;s performance metrics. Transform <cite>Y</cite> and <cite>T</cite> into scikit&#8217;s
format by <code class="docutils literal"><span class="pre">y_true</span> <span class="pre">=</span> <span class="pre">T.argmax[1]</span></code>, <code class="docutils literal"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">Y.argmax[1]</span></code>.
<a class="reference external" href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics">http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>T</strong> (<em>matrix</em>) &#8211; true outputs.</li>
<li><strong>Y</strong> (<em>matrix</em>) &#8211; ELM model predictions, can be computed with <cite>predict()</cite> function.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>e</strong> &#8211;
MSE for regression / classification error for classification.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">double</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>fname</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load ELM model data from a file.</p>
<p>Load requires an <code class="docutils literal"><span class="pre">ELM</span></code> object, and it uses solver type, precision and batch size from that ELM object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>fname</strong> (<em>string</em>) &#8211; filename to load model from.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict outputs Y for the given input data X.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>matrix</em>) &#8211; input data of size (N * <cite>inputs</cite>)</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>Y</strong> &#8211;
output data or predicted classes, size (N * <cite>outputs</cite>).</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">matrix</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.project">
<code class="descname">project</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.project"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.project" title="Permalink to this definition">¶</a></dt>
<dd><p>Get ELM&#8217;s hidden layer representation of input data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>matrix</em>) &#8211; input data, size (N * <cite>inputs</cite>)</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>H</strong> &#8211;
hidden layer representation matrix, size (N * number_of_neurons)</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">matrix</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>fname</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save ELM model with current parameters.</p>
<p>Model does not save a particular solver, precision batch size. They are obtained from
a new ELM when loading the model (so one can switch to another solver, for instance).</p>
<p>Also ranking and max number of neurons are not saved, because they
are runtime training info irrelevant after the training completes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>fname</strong> (<em>string</em>) &#8211; filename to save model into.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.elm.ELM.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>X</em>, <em>T</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/elm.html#ELM.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.elm.ELM.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Universal training interface for ELM model with model structure selection.</p>
<p>Model structure selection takes more time and requires all data to fit into memory. Optimal pruning (&#8216;OP&#8217;,
effectively an L1-regularization) takes the most time but gives the smallest and best performing model.
Choosing a classification forces ELM to use classification error in model structure selection,
and in <cite>error()</cite> method output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>matrix</em>) &#8211; input data matrix, size (N * <cite>inputs</cite>)</li>
<li><strong>T</strong> (<em>matrix</em>) &#8211; outputs data matrix, size (N * <cite>outputs</cite>)</li>
<li><strong>'V'/'CV'/'LOO'</strong> (<em>sting, choose one</em>) &#8211; model structure selection: select optimal number of neurons using
a validation set (&#8216;V&#8217;), cross-validation (&#8216;CV&#8217;) or Leave-One-Out (&#8216;LOO&#8217;)</li>
<li><strong>'OP'</strong> (<em>string, use with &#8216;V&#8217;/&#8217;CV&#8217;/&#8217;LOO&#8217;</em>) &#8211; choose best neurons instead of random ones, training takes longer;
equivalent to L1-regularization</li>
<li><strong>'c'/'wc'/'ml'/'r'</strong> (<em>string, choose one</em>) &#8211; train ELM for classification (&#8216;c&#8217;), classification with weighted
classes (&#8216;wc&#8217;), multi-label classification (&#8216;ml&#8217;) with several correct classes per data sample, or
regression (&#8216;r&#8217;) without any classification. In classification, number of <cite>outputs</cite> is the number
of classes; correct class(es) for each sample has value 1 and incorrect classes have 0.
Overwrites parameters given an ELM initialization time.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Keyword Arguments:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>Xv</strong> (<em>matrix, use with &#8216;V&#8217;</em>) &#8211;
validation set input data, size (Nv * <cite>inputs</cite>)</li>
<li><strong>Tv</strong> (<em>matrix, use with &#8216;V&#8217;</em>) &#8211;
validation set outputs data, size (Nv * <cite>outputs</cite>)</li>
<li><strong>k</strong> (<em>int, use with &#8216;CV&#8217;</em>) &#8211;
number of splits for cross-validation, k&gt;=3</li>
<li><strong>kmax</strong> (<em>int, optional, use with &#8216;OP&#8217;</em>) &#8211;
maximum number of neurons to keep in ELM</li>
<li><strong>batch</strong> (<em>int, optional</em>) &#8211;
batch size for ELM, overwrites batch size from the initialization</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-hpelm.hp_elm">
<span id="hpelm-hp-elm-module"></span><h2>hpelm.hp_elm module<a class="headerlink" href="#module-hpelm.hp_elm" title="Permalink to this headline">¶</a></h2>
<p>Created on Mon Oct 27 17:48:33 2014</p>
<p>&#64;author: akusok</p>
<dl class="class">
<dt id="hpelm.hp_elm.HPELM">
<em class="property">class </em><code class="descclassname">hpelm.hp_elm.</code><code class="descname">HPELM</code><span class="sig-paren">(</span><em>inputs</em>, <em>outputs</em>, <em>classification=''</em>, <em>w=None</em>, <em>batch=1000</em>, <em>accelerator=None</em>, <em>precision='double'</em>, <em>norm=None</em>, <em>tprint=5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hpelm.elm.ELM" title="hpelm.elm.ELM"><code class="xref py py-class docutils literal"><span class="pre">hpelm.elm.ELM</span></code></a></p>
<p>Interface for training High-Performance Extreme Learning Machines (HP-ELM).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>inputs</strong> (<em>int</em>) &#8211; dimensionality of input data, or number of data features</li>
<li><strong>outputs</strong> (<em>int</em>) &#8211; dimensionality of output data, or number of classes</li>
<li><strong>classification</strong> (<em>&#8216;c&#8217;/&#8217;wc&#8217;/&#8217;ml&#8217;, optional</em>) &#8211; train ELM for classfication (&#8216;c&#8217;) / weighted classification (&#8216;wc&#8217;) /
multi-label classification (&#8216;ml&#8217;). For weighted classification you can provide weights in <cite>w</cite>. ELM will
compute and use the corresponding classification error instead of Mean Squared Error.</li>
<li><strong>w</strong> (<em>vector, optional</em>) &#8211; weights vector for weighted classification, lenght (<cite>outputs</cite> * 1).</li>
<li><strong>batch</strong> (<em>int, optional</em>) &#8211; batch size for data processing in ELM, reduces memory requirements. Does not work
for model structure selection (validation, cross-validation, Leave-One-Out). Can be changed later directly
as a class attribute.</li>
<li><strong>accelerator</strong> (<em>string, optional</em>) &#8211; type of accelerated ELM to use: None, &#8216;GPU&#8217;, ...</li>
<li><strong>precision</strong> (<em>optional</em>) &#8211; data precision to use, supports signle (&#8216;single&#8217;, &#8216;32&#8217; or numpy.float32) or double
(&#8216;double&#8217;, &#8216;64&#8217; or numpy.float64). Single precision is faster but may cause numerical errors. Majority
of GPUs work in single precision. Default: <strong>double</strong>.</li>
<li><strong>norm</strong> (<em>double, optinal</em>) &#8211; L2-normalization parameter, <strong>None</strong> gives the default value.</li>
<li><strong>tprint</strong> (<em>int, optional</em>) &#8211; ELM reports its progess every <cite>tprint</cite> seconds or after every batch,
whatever takes longer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Class attributes; attributes that simply store initialization or <cite>train()</cite> parameters are omitted.</p>
<dl class="attribute">
<dt id="hpelm.hp_elm.HPELM.nnet">
<code class="descname">nnet</code><a class="headerlink" href="#hpelm.hp_elm.HPELM.nnet" title="Permalink to this definition">¶</a></dt>
<dd><p><em>object</em></p>
<p>Implementation of neural network with computational methods, but without
complex logic. Different implementations are given by different classes: for Python, for GPU, etc.
See <code class="docutils literal"><span class="pre">hpelm.nnets</span></code> folder for particular files. You can implement your own computational algorithm
by inheriting from <code class="docutils literal"><span class="pre">hpelm.nnets.SLFN</span></code> and overwriting some methods.</p>
</dd></dl>

<dl class="attribute">
<dt id="hpelm.hp_elm.HPELM.flist">
<code class="descname">flist</code><a class="headerlink" href="#hpelm.hp_elm.HPELM.flist" title="Permalink to this definition">¶</a></dt>
<dd><p><em>list of strings</em></p>
<p>Awailable types of neurons, use them when adding new neurons.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The &#8216;hdf5&#8217; type denotes a name of HDF5 file type with a single 2-dimensional array inside. HPELM uses PyTables
interface to HDF5: <a class="reference external" href="http://www.pytables.org/">http://www.pytables.org/</a>. For HDF5 array examples, see
<a class="reference external" href="http://www.pytables.org/usersguide/libref/homogenous_storage.html">http://www.pytables.org/usersguide/libref/homogenous_storage.html</a>. Array name is irrelevant,
but there must be <strong>only one array per HDF5 file</strong>.</p>
<p class="last">A 2-dimensional Numpy.ndarray can also be used.</p>
</div>
<dl class="method">
<dt id="hpelm.hp_elm.HPELM.add_data">
<code class="descname">add_data</code><span class="sig-paren">(</span><em>fX</em>, <em>fT</em>, <em>istart=0</em>, <em>icount=inf</em>, <em>fHH=None</em>, <em>fHT=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.add_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.add_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Feed new training data (X,T) to HP-ELM model in batches: does not solve ELM itself.</p>
<p>This method prepares an intermediate solution data, that takes the most time. After that, obtaining
the solution is fast.</p>
<p>The intermediate solution consists of two matrices: <cite>HH</cite> and <cite>HT</cite>. They can be in memory for a model computed
at once, or stored on disk for a model computed in parts or in parallel.</p>
<p>For iterative solution, provide file names for on-disk matrices in the input parameters <cite>fHH</cite> and <cite>fHT</cite>.
They will be created if they don&#8217;t exist, or new results will be merged with the existing ones. This method is
multiprocess-safe for parallel writing into files <cite>fHH</cite> and <cite>fHT</cite>, that allows you to easily compute ELM
in parallel. The multiprocess-safeness uses Python module &#8216;fasteners&#8217; and a lock file, which is named
fHH+&#8217;.lock&#8217; and fHT+&#8217;.lock&#8217;.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>fX</strong> (<em>hdf5</em>) &#8211; (part of) input training data size (N * <cite>inputs</cite>)</li>
<li><strong>(hdf5) (part of) output training data size (N * outputs)</strong> (<em>fT</em>) &#8211; </li>
<li><strong>istart</strong> (<em>int, optional</em>) &#8211; index of first data sample to use from <cite>fX</cite>, <cite>istart</cite> &lt; N. If not given,
all data from <cite>fX</cite> is used. Sample with index <cite>istart</cite> is used for training, indexing is 0-based.</li>
<li><strong>icount</strong> (<em>int, optional</em>) &#8211; number of data samples to use from <cite>fX</cite>, starting from <cite>istart</cite>, automatically
adjusted to <cite>istart</cite> + <cite>icount</cite> &lt;= N. If not given, all data starting from <cite>start</cite> is used.
The last sample used for training is <cite>istart`+`icount</cite>-1, so you can index data as:
istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...</li>
<li><strong>fHT</strong> (<em>fHH,</em>) &#8211; file names for storing HH and HT matrices. Files are created if they don&#8217;t
exist, or new result is added to the existing files if they exist. Parallel writing to the same
<cite>fHH</cite>, <cite>fHT</cite> files is multiprocess-safe, made specially for parallel training of HP-ELM. Another use
is to split a very long training of huge ELM into smaller parts, so the training can be interrupted
and resumed later.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.add_data_async">
<code class="descname">add_data_async</code><span class="sig-paren">(</span><em>fX</em>, <em>fT</em>, <em>istart=0</em>, <em>icount=inf</em>, <em>fHH=None</em>, <em>fHT=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.add_data_async"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.add_data_async" title="Permalink to this definition">¶</a></dt>
<dd><p>Version of <cite>add_data()</cite> with asyncronous I/O. See <cite>add_data()</cite> for reference.</p>
<p>Spawns new processes using Python&#8217;s <cite>multiprocessing</cite> module, and requires more memory than non-async version.</p>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.error">
<code class="descname">error</code><span class="sig-paren">(</span><em>fT</em>, <em>fY</em>, <em>istart=0</em>, <em>icount=inf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.error"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.error" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate error of model predictions of HPELM.</p>
<p>Computes Mean Squared Error (MSE) between model predictions Y and true outputs T.
For classification, computes mis-classification error.
For multi-label classification, correct classes are all with Y&gt;0.5.</p>
<p>For weighted classification the error is an average weighted True Positive Rate,
or percentage of correctly predicted samples for each class, multiplied by weight
of that class and averaged. If you want something else, just write it yourself :)
See <a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">https://en.wikipedia.org/wiki/Confusion_matrix</a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>fT</strong> (<em>hdf5</em>) &#8211; hdf5 filename with true outputs</li>
<li><strong>fY</strong> (<em>hdf5</em>) &#8211; hdf5 filename with predicted outputs</li>
<li><strong>istart</strong> (<em>int, optional</em>) &#8211; index of first data sample to use from <cite>fX</cite>, <cite>istart</cite> &lt; N. If not given,
all data from <cite>fX</cite> is used. Sample with index <cite>istart</cite> is used for training, indexing is 0-based.</li>
<li><strong>icount</strong> (<em>int, optional</em>) &#8211; number of data samples to use from <cite>fX</cite>, starting from <cite>istart</cite>, automatically
adjusted to <cite>istart</cite> + <cite>icount</cite> &lt;= N. If not given, all data starting from <cite>start</cite> is used.
The last sample used for training is <cite>istart`+`icount</cite>-1, so you can index data as:
istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>e</strong> &#8211;
MSE for regression / classification error for classification.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">double</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>fX</em>, <em>fY</em>, <em>istart=0</em>, <em>icount=inf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterative predict outputs and save them to HDF5, can use custom range.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>fX</strong> (<em>hdf5</em>) &#8211; hdf5 filename with input data from which outputs are predicted</li>
<li><strong>fY</strong> (<em>hdf5</em>) &#8211; hdf5 filename to store output data into</li>
<li><strong>istart</strong> (<em>int, optional</em>) &#8211; index of first data sample to use from <cite>fX</cite>, <cite>istart</cite> &lt; N. If not given,
all data from <cite>fX</cite> is used. Sample with index <cite>istart</cite> is used for training, indexing is 0-based.</li>
<li><strong>icount</strong> (<em>int, optional</em>) &#8211; number of data samples to use from <cite>fX</cite>, starting from <cite>istart</cite>, automatically
adjusted to <cite>istart</cite> + <cite>icount</cite> &lt;= N. If not given, all data starting from <cite>start</cite> is used.
The last sample used for training is <cite>istart`+`icount</cite>-1, so you can index data as:
istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.predict_async">
<code class="descname">predict_async</code><span class="sig-paren">(</span><em>fX</em>, <em>fY</em>, <em>istart=0</em>, <em>icount=inf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.predict_async"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.predict_async" title="Permalink to this definition">¶</a></dt>
<dd><p>Version of <cite>predict()</cite> with asyncronous I/O. See <cite>predict()</cite> for reference.</p>
<p>Spawns new processes using Python&#8217;s <cite>multiprocessing</cite> module, and requires more memory than non-async version.</p>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.project">
<code class="descname">project</code><span class="sig-paren">(</span><em>fX</em>, <em>fH</em>, <em>istart=0</em>, <em>icount=inf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.project"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.project" title="Permalink to this definition">¶</a></dt>
<dd><p>Iteratively project input data from HDF5 into HPELM hidden layer, and save in another HDF5.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>fX</strong> (<em>hdf5</em>) &#8211; hdf5 filename with input data from which outputs are predicted</li>
<li><strong>fH</strong> (<em>hdf5</em>) &#8211; hdf5 filename to store output data into</li>
<li><strong>istart</strong> (<em>int, optional</em>) &#8211; index of first data sample to use from <cite>fX</cite>, <cite>istart</cite> &lt; N. If not given,
all data from <cite>fX</cite> is used. Sample with index <cite>istart</cite> is used for training, indexing is 0-based.</li>
<li><strong>icount</strong> (<em>int, optional</em>) &#8211; number of data samples to use from <cite>fX</cite>, starting from <cite>istart</cite>, automatically
adjusted to <cite>istart</cite> + <cite>icount</cite> &lt;= N. If not given, all data starting from <cite>start</cite> is used.
The last sample used for training is <cite>istart`+`icount</cite>-1, so you can index data as:
istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.solve_corr">
<code class="descname">solve_corr</code><span class="sig-paren">(</span><em>fHH</em>, <em>fHT</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.solve_corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.solve_corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Solves an ELM model with the given (covariance) fHH and (correlation) fHT HDF5 files.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>fHH</strong> (<em>hdf5</em>) &#8211; an hdf5 file with intermediate solution data</li>
<li><strong>fHT</strong> (<em>hdf5</em>) &#8211; an hdf5 file with intermediate solution data</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>fX</em>, <em>fT</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Universal training interface for HP-ELM model.</p>
<p>Always trains a basic ELM model without model structure selection.
L2-regularization is available as <cite>norm</cite> parameter at HPELM initialization.
Number of neurons selection with validation set for trained HPELM is available in <cite>train_hpv()</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>fX</strong> (<em>hdf5</em>) &#8211; input data on disk, size (N * <cite>inputs</cite>)</li>
<li><strong>fT</strong> (<em>hdf5</em>) &#8211; outputs data on disk, size (N * <cite>outputs</cite>)</li>
<li><strong>'c'/'wc'/'ml'</strong> (<em>string, choose one</em>) &#8211; train HPELM for classification (&#8216;c&#8217;), classification with weighted
classes (&#8216;wc&#8217;) or multi-label classification (&#8216;ml&#8217;) with several correct classes per data sample.
In classification, number of <cite>outputs</cite> is the number of classes; correct class(es) for each sample
has value 1 and incorrect classes have 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Keyword Arguments:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>istart</strong> (<em>int, optional</em>) &#8211;
index of first data sample to use from <cite>fX</cite>, <cite>istart</cite> &lt; N. If not given,
all data from <cite>fX</cite> is used. Sample with index <cite>istart</cite> is used for training, indexing is 0-based.</li>
<li><strong>icount</strong> (<em>int, optional</em>) &#8211;
number of data samples to use from <cite>fX</cite>, starting from <cite>istart</cite>, automatically
adjusted to <cite>istart</cite> + <cite>icount</cite> &lt;= N. If not given, all data starting from <cite>start</cite> is used.
The last sample used for training is <cite>istart`+`icount</cite>-1, so you can index data as:
istart_1=0, icount_1=1000; istart_2=1000, icount_2=1000; istart_3=2000, icount_3=1000, ...</li>
<li><strong>batch</strong> (<em>int, optional</em>) &#8211;
batch size for ELM, overwrites batch size from the initialization</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.train_async">
<code class="descname">train_async</code><span class="sig-paren">(</span><em>fX</em>, <em>fT</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.train_async"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.train_async" title="Permalink to this definition">¶</a></dt>
<dd><p>Training HPELM with asyncronous I/O, good for network drives, etc. See <cite>train()</cite> for reference.</p>
<p>Spawns new processes using Python&#8217;s <cite>multiprocessing</cite> module.</p>
</dd></dl>

<dl class="method">
<dt id="hpelm.hp_elm.HPELM.validation_corr">
<code class="descname">validation_corr</code><span class="sig-paren">(</span><em>fHH</em>, <em>fHT</em>, <em>fXv</em>, <em>fTv</em>, <em>steps=10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/hp_elm.html#HPELM.validation_corr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.hp_elm.HPELM.validation_corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Quick batch error evaluation with different numbers of neurons on a validation set.</p>
<p>Only feasible implementation of model structure selection with HP-ELM. This method makes a single pass
over the validation data, computing errors for all numbers of neurons at once. It requires HDF5 files with
matrices HH and HT: <cite>fHH</cite> and <cite>fHT</cite>, obtained from <cite>add_data(..., fHH, fHT)</cite> method.</p>
<p>The method writes the best solution to the HPELM model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>fHH</strong> (<em>string</em>) &#8211; name of HDF5 file with HH matrix</li>
<li><strong>fHT</strong> (<em>string</em>) &#8211; name of HDF5 file with HT matrix</li>
<li><strong>fXv</strong> (<em>string</em>) &#8211; name of HDF5 file with validation dataset inputs</li>
<li><strong>fTv</strong> (<em>string</em>) &#8211; name of HDF5 file with validation dataset outputs</li>
<li><strong>steps</strong> (<em>int or vector</em>) &#8211; amount of different numbers of neurons to test, choosen uniformly on a logarithmic
scale from 3 to number of neurons in HPELM. Can be given exactly as a vector.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p><strong>Ls</strong> &#8211;
numbers of neurons used by <cite>validation_corr()</cite> method
errs (vector): corresponding errors for number of neurons in <cite>Ls</cite>, with classification error if model</p>
<blockquote>
<div><p>is run for classification</p>
</div></blockquote>
<p>confs (list of matrix): list of confusion matrices corresponding to elements in Ls (empty for regression)</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">vector</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-hpelm.mss_cv">
<span id="hpelm-mss-cv-module"></span><h2>hpelm.mss_cv module<a class="headerlink" href="#module-hpelm.mss_cv" title="Permalink to this headline">¶</a></h2>
<p>Created on Mon Oct 27 17:48:33 2014</p>
<p>&#64;author: akusok</p>
<dl class="function">
<dt id="hpelm.mss_cv.train_cv">
<code class="descclassname">hpelm.mss_cv.</code><code class="descname">train_cv</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>T</em>, <em>k</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/mss_cv.html#train_cv"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.mss_cv.train_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Model structure selection with cross-validation.</p>
<p>Trains ELM, cross-validates model and sets an optimal validated solution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<a class="reference internal" href="#hpelm.elm.ELM" title="hpelm.elm.ELM"><em>ELM</em></a>) &#8211; ELM object that calls <cite>train_v()</cite></li>
<li><strong>X</strong> (<em>matrix</em>) &#8211; training set inputs</li>
<li><strong>T</strong> (<em>matrix</em>) &#8211; training set outputs</li>
<li><strong>k</strong> (<em>int</em>) &#8211; number of parts to split the dataset into, k-2 parts are used for training and 2 parts are
left out: 1 for validation and 1 for test; repeated k times until all parts have been left out for
validation and test, and results averaged over these k repetitions.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p><strong>err_t</strong> &#8211;
error for the optimal model, computed in the &#8216;cross-testing&#8217; manner on data part</p>
<blockquote>
<div><p>which is not used for training or validation</p>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">double</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="hpelm-mss-hpv-module">
<h2>hpelm.mss_hpv module<a class="headerlink" href="#hpelm-mss-hpv-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-hpelm.mss_loo">
<span id="hpelm-mss-loo-module"></span><h2>hpelm.mss_loo module<a class="headerlink" href="#module-hpelm.mss_loo" title="Permalink to this headline">¶</a></h2>
<p>Created on Mon Oct 27 17:48:33 2014</p>
<p>&#64;author: akusok</p>
<dl class="function">
<dt id="hpelm.mss_loo.train_loo">
<code class="descclassname">hpelm.mss_loo.</code><code class="descname">train_loo</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>T</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/mss_loo.html#train_loo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.mss_loo.train_loo" title="Permalink to this definition">¶</a></dt>
<dd><p>Model structure selection with Leave-One-Out (LOO) validation.</p>
<p>Trains ELM, validates model with LOO and sets an optimal validated solution. Effect is similar to
cross-validation with k==N, but ELM has explicit formula of solution for LOO without iterating k times.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>self</strong> (<a class="reference internal" href="#hpelm.elm.ELM" title="hpelm.elm.ELM"><em>ELM</em></a>) &#8211; ELM object that calls <cite>train_v()</cite></li>
<li><strong>X</strong> (<em>matrix</em>) &#8211; training set inputs</li>
<li><strong>T</strong> (<em>matrix</em>) &#8211; training set outputs</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-hpelm.mss_v">
<span id="hpelm-mss-v-module"></span><h2>hpelm.mss_v module<a class="headerlink" href="#module-hpelm.mss_v" title="Permalink to this headline">¶</a></h2>
<p>Created on Mon Oct 27 17:48:33 2014</p>
<p>&#64;author: akusok</p>
<dl class="function">
<dt id="hpelm.mss_v.train_v">
<code class="descclassname">hpelm.mss_v.</code><code class="descname">train_v</code><span class="sig-paren">(</span><em>self</em>, <em>X</em>, <em>T</em>, <em>Xv</em>, <em>Tv</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hpelm/mss_v.html#train_v"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hpelm.mss_v.train_v" title="Permalink to this definition">¶</a></dt>
<dd><p>Model structure selection with a validation set.</p>
<p>Trains ELM, validates model and sets an optimal validated solution.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>self</strong> (<a class="reference internal" href="#hpelm.elm.ELM" title="hpelm.elm.ELM"><em>ELM</em></a>) &#8211; ELM object that calls <cite>train_v()</cite></li>
<li><strong>X</strong> (<em>matrix</em>) &#8211; training set inputs</li>
<li><strong>T</strong> (<em>matrix</em>) &#8211; training set outputs</li>
<li><strong>Xv</strong> (<em>matrix</em>) &#8211; validation set inputs</li>
<li><strong>Tv</strong> (<em>matrix</em>) &#8211; validation set outputs</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-hpelm">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-hpelm" title="Permalink to this headline">¶</a></h2>
<p>Created on Aug 18, 2014</p>
<p>&#64;author: akusoka1</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="hpelm.modules.html" class="btn btn-neutral float-right" title="hpelm.modules package">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Welcome to hpelm’s documentation!"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Anton Akusok.
    </p>
  </div>

  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.6.22',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>